{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE8emQONkPZ7"
      },
      "outputs": [],
      "source": [
        "#Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OrgM-uk6kYf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre"
      ],
      "metadata": {
        "id": "60FHdLbfkbdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "corpus_name = \"cornell\"\n",
        "corpus = os.path.join(\"/content/gdrive/My Drive/data\", corpus_name)\n",
        "\n",
        "def printLines(file, n=10):\n",
        "    with open(file, 'rb') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(corpus, \"utterances.jsonl\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95XcOhmpkcnX",
        "outputId": "d604ced9-8e67-4704-d08a-8e25e2c57bc6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "b'{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let\\'s go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"\\'s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L924\", \"conversation_id\": \"L924\", \"text\": \"Wow\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Wow\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L872\", \"conversation_id\": \"L870\", \"text\": \"Okay -- you\\'re gonna need to learn how to lie.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 4, \"toks\": [{\"tok\": \"Okay\", \"tag\": \"UH\", \"dep\": \"intj\", \"up\": 4, \"dn\": []}, {\"tok\": \"--\", \"tag\": \":\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"\\'re\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"gon\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 6, 12]}, {\"tok\": \"na\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 6, \"dn\": []}, {\"tok\": \"need\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 8]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 8, \"dn\": []}, {\"tok\": \"learn\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 6, \"dn\": [7, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 11, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 11, \"dn\": []}, {\"tok\": \"lie\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 8, \"dn\": [9, 10]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": \"L871\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L871\", \"conversation_id\": \"L870\", \"text\": \"No\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"No\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": \"L870\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L870\", \"conversation_id\": \"L870\", \"text\": \"I\\'m kidding.  You know how sometimes you just become this \\\\\"persona\\\\\"?  And you don\\'t know how to quit?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 2, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"\\'m\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 2, \"dn\": []}, {\"tok\": \"kidding\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 3]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 2, \"dn\": [4]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 3, \"dn\": []}]}, {\"rt\": 1, \"toks\": [{\"tok\": \"You\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 6, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 3, \"dn\": []}, {\"tok\": \"sometimes\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": [2]}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 6, \"dn\": []}, {\"tok\": \"just\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": []}, {\"tok\": \"become\", \"tag\": \"VBP\", \"dep\": \"ccomp\", \"up\": 1, \"dn\": [3, 4, 5, 9]}, {\"tok\": \"this\", \"tag\": \"DT\", \"dep\": \"det\", \"up\": 9, \"dn\": []}, {\"tok\": \"\\\\\"\", \"tag\": \"``\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"persona\", \"tag\": \"NN\", \"dep\": \"attr\", \"up\": 6, \"dn\": [7, 8, 10]}, {\"tok\": \"\\\\\"\", \"tag\": \"\\'\\'\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": [12]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 11, \"dn\": []}]}, {\"rt\": 4, \"toks\": [{\"tok\": \"And\", \"tag\": \"CC\", \"dep\": \"cc\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"n\\'t\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 4, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 7, 8]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 7, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 7, \"dn\": []}, {\"tok\": \"quit\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 6]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L869\", \"conversation_id\": \"L866\", \"text\": \"Like my fear of wearing pastels?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Like\", \"tag\": \"IN\", \"dep\": \"ROOT\", \"dn\": [2, 6]}, {\"tok\": \"my\", \"tag\": \"PRP$\", \"dep\": \"poss\", \"up\": 2, \"dn\": []}, {\"tok\": \"fear\", \"tag\": \"NN\", \"dep\": \"pobj\", \"up\": 0, \"dn\": [1, 3]}, {\"tok\": \"of\", \"tag\": \"IN\", \"dep\": \"prep\", \"up\": 2, \"dn\": [4]}, {\"tok\": \"wearing\", \"tag\": \"VBG\", \"dep\": \"pcomp\", \"up\": 3, \"dn\": [5]}, {\"tok\": \"pastels\", \"tag\": \"NNS\", \"dep\": \"dobj\", \"up\": 4, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L868\", \"timestamp\": null, \"vectors\": []}\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper Function class \n",
        "  #We will set the length of sentence that we will consider\n",
        "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
        "class SetVocab:\n",
        "  def __init__(self,vocab,corpus, corpus_name, datafile):\n",
        "    self.vocab = vocab\n",
        "    self.corpus = corpus\n",
        "    self.datafile = datafile\n",
        "    self.corpus_name = corpus_name\n",
        "\n",
        "  def unicodeToAscii(self,s):\n",
        "      return ''.join(\n",
        "          c for c in unicodedata.normalize('NFD', s)\n",
        "          if unicodedata.category(c) != 'Mn'\n",
        "      )\n",
        "\n",
        "  # Lowercase, trim, and remove non-letter characters\n",
        "  def normalizeString(self,s):\n",
        "      s = self.unicodeToAscii(s.lower().strip())\n",
        "      s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "      s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "      s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "      return s\n",
        "\n",
        "  # Read query/response pairs and return a voc object\n",
        "  def readVocs(self):\n",
        "      print(\"Reading lines...\")\n",
        "      # Read the file and split into lines\n",
        "      lines = open(self.datafile, encoding='utf-8').\\\n",
        "          read().strip().split('\\n')\n",
        "      # Split every line into pairs and normalize\n",
        "      pairs = [[self.normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "      voc = self.vocab(self.corpus_name)\n",
        "      return voc, pairs\n",
        "\n",
        "  # Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
        "  def filterPair(self,p):\n",
        "      # Input sequences need to preserve the last word for EOS token\n",
        "      return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "  # Filter pairs using filterPair condition\n",
        "  def filterPairs(self,pairs):\n",
        "      return [pair for pair in pairs if self.filterPair(pair)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nHjNEQ1dkkSC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default word tokens\n",
        "UNK_token = 0\n",
        "PAD_token = 1  # Used for padding short sentences\n",
        "SOS_token = 2  # Start-of-sentence token\n",
        "EOS_token = 3  # End-of-sentence token\n",
        "# Create default tokens, these will be used to pad or signal\n",
        "\n",
        "\n",
        "#Build a chatbot vocabulary based on the word corpus that we have \n",
        "class ChatbotVocab:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        # we will define later\n",
        "        self.trimmed = False\n",
        "        #create dictionaries to store the index and the count for each word in the corpus\n",
        "        self.maptoindex = {}\n",
        "        self.maptocount = {}\n",
        "        #map index to word for faster retrieval\n",
        "        self.index2word = {UNK_token: \"UNK\",PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count SOS, EOS, PAD, consider unique only\n",
        "        self.trimmed = False\n",
        "\n",
        "    #any time we pass a sentence to the vocah\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    #called by the sentence for each word it hass\n",
        "    def addWord(self, word):\n",
        "      \"\"\"\n",
        "      Check if the word is in the corpus\n",
        "      If not create new identity for it\n",
        "      create an index for it, set count\n",
        "      Increment the number of words in vocab\n",
        "      \"\"\"\n",
        "      if word not in self.maptoindex:\n",
        "\n",
        "          self.maptoindex[word] = self.num_words\n",
        "          self.maptocount[word] = 1\n",
        "          self.index2word[self.num_words] = word\n",
        "          self.num_words += 1\n",
        "      else:\n",
        "          self.maptocount[word] += 1\n",
        "\n",
        "    # Avoid noise by trimming certain words that are rare\n",
        "    def trim(self, min_count=3):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.maptocount.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('Keeping only these many words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.maptoindex), len(keep_words) / len(self.maptoindex)\n",
        "        ))\n",
        "\n",
        "        # Reset and add all non trimmed words again dictionaries\n",
        "        self.maptoindex = {}\n",
        "        self.maptocount = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3 # Count default tokens\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n",
        "\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    # Trim words used under the MIN_COUNT from the voc\n",
        "    \n",
        "    voc.trim(MIN_COUNT)\n",
        "    # Filter out pairs with trimmed words\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        # Check input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.maptoindex:\n",
        "                keep_input = False\n",
        "                break\n",
        "        # Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.maptoindex:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed to {} pairs\".format(len(keep_pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "# Using the functions defined above, return a populated voc object and pairs list\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    #Create the Vocab Helper Class by \n",
        "    setup_voc = SetVocab(ChatbotVocab,corpus, corpus_name,datafile)\n",
        "    voc, pairs = setup_voc.readVocs()\n",
        "    pairs = setup_voc.filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
        "    #trim the pairs\n",
        "    pairs = trimRareWords(voc, pairs,MIN_COUNT)\n",
        "    return voc, pairs\n",
        "\n",
        "# Setup FilePath\n",
        "# Upload the file in data/cornell/ in gooogle drive\n",
        "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
        "delimiter = '\\t'\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
        "print(f'Words in our courpus {voc.num_words}')\n",
        "print(f'Number of pairs {len(pairs)}')\n",
        "# Print some pairs to validate\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:5]:\n",
        "    print(pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpSW4OTaksvD",
        "outputId": "990a2965-98da-4812-e753-2153f2cf6048"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Trimmed to 64313 sentence pairs\n",
            "Counting words...\n",
            "Keeping only these many words 7833 / 18079 = 0.4333\n",
            "Trimmed to 53131 pairs\n",
            "Words in our courpus 7836\n",
            "Number of pairs 53131\n",
            "\n",
            "pairs:\n",
            "['they do to !', 'they do not !']\n",
            "['she okay ?', 'i hope so .']\n",
            "['wow', 'let s go .']\n",
            "['what good stuff ?', 'the real you .']\n",
            "['do you listen to this crap ?', 'what crap ?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(voc.num_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMEX98cPmqSe",
        "outputId": "2fffaf48-0097-4f32-ea3a-bc42e5fed6a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a batch from the sequences\n",
        "def sent2index(voc, sentence):\n",
        "    #return the index of each word in the corpus\n",
        "    return [voc.maptoindex[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "\n",
        "def Padding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMask(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputBatch(l, voc):\n",
        "    indexes_batch = [sent2index(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = Padding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputBathc(l, voc):\n",
        "    indexes_batch = [sent2index(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = Padding(indexes_batch)\n",
        "    mask = binaryMask(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# Returns all items for a given batch of pairs\n",
        "# Returns all items for a given batch of pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputBatch(input_batch, voc)\n",
        "    output, mask, max_target_len = outputBathc(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n"
      ],
      "metadata": {
        "id": "4A71HZ0Wmx4e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-MkA3WLn0Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "Bx9tF00En5vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Transformer\n",
        "#helpers\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    # Define special symbols and indices\n",
        "    UNK_token, PAD_token, SOS_token, EOS_token =  0,1, 2, 3\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_token).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_token).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "iNIO6b4n3_4n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#helpers\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    # Define special symbols and indices\n",
        "    UNK_token, PAD_token, SOS_token, EOS_token =  0,1, 2, 3\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_token).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_token).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "MpQ_dIV6pZNH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "        \n",
        "    def translate(self, src: Tensor, src_mask: Tensor, max_length: int) -> Tensor:\n",
        "        \"\"\"Translate the given source sequence into target sequence.\"\"\"\n",
        "        enc_outputs = self.encode(src, src_mask)\n",
        "        # Create a tensor to hold the predicted output sequence\n",
        "        tgt = torch.zeros(1, 1).fill_(BOS_IDX).type_as(src)\n",
        "        for i in range(max_length):\n",
        "            tgt_mask = (generate_square_subsequent_mask(tgt.size(0))\n",
        "                        .type(torch.bool)).type_as(src_mask)\n",
        "            # Predict the next word in the target sequence\n",
        "            with torch.no_grad():\n",
        "                output = self.decode(tgt, enc_outputs, tgt_mask)\n",
        "                pred = self.generator(output[:, -1])\n",
        "            # Append the predicted word to the output sequence\n",
        "            _, next_word = torch.max(pred, dim=1)\n",
        "            next_word = next_word.item()\n",
        "            tgt = torch.cat([tgt, torch.zeros(1, 1).fill_(next_word).type_as(src)], dim=0)\n",
        "            # If the predicted word is the end-of-sentence token, stop generating\n",
        "            if next_word == EOS_IDX:\n",
        "                break\n",
        "        # Remove the start-of-sentence and end-of-sentence tokens from the output sequence\n",
        "        return tgt[1:-1]\n",
        "    "
      ],
      "metadata": {
        "id": "s7PBgqD1n_u4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Custom Dataset and Dataloader\n",
        "pairs[1][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_2m2sVn6pbCl",
        "outputId": "b65bfd99-df55-45c3-8501-49521ee9a8db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i hope so .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Custom Dataset and Dataloader"
      ],
      "metadata": {
        "id": "BIVf46E2Ng-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into trian and validate\n",
        "validation_set_size = 0.2 \n",
        "dataset_size = len(pairs)\n",
        "validation_size = int(validation_set_size * dataset_size)\n",
        "train_size = dataset_size - validation_size"
      ],
      "metadata": {
        "id": "NXIo5tq8arM3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "import torch\n",
        "#Create a Dataloader\n",
        "class Dataset_pairs(torch.utils.data.Dataset):\n",
        "  'Make a dataset from the already existing pairs'\n",
        "  def __init__(self, pairs):\n",
        "        'Initialization'\n",
        "        self.pairs = pairs\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.pairs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "\n",
        "        pair = self.pairs[index]\n",
        "        # Load data and get label\n",
        "        \n",
        "\n",
        "        return pair \n",
        "\n",
        "CustomData = Dataset_pairs(pairs)\n",
        "\n",
        "#The pairs from dataloader will be sent to the collate function, we will \n",
        "#get the desired output from here\n",
        "def collate_fn(batch):\n",
        "    op = batch2TrainData(voc,batch)\n",
        "    return op\n",
        "\n",
        "# Split the dataset into training and validation subsets\n",
        "train_dataset, val_dataset = random_split(CustomData, [train_size, validation_size])\n",
        "\n",
        "# Define batch sizes for training and validation\n",
        "batch_size = 64\n",
        "# Create data loaders for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "qAcAl0Q0Ncfo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training"
      ],
      "metadata": {
        "id": "iqlzuDHjpfVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Parameters\n",
        "SRC_VOCAB_SIZE = voc.num_words\n",
        "TGT_VOCAB_SIZE = voc.num_words\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 2\n",
        "NUM_DECODER_LAYERS = 2\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "UNK_token, PAD_token, SOS_token, EOS_token = 0, 1, 2, 3\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.001 )"
      ],
      "metadata": {
        "id": "EYO_n_g7phM7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "#                   ]\n",
        "# print('tr')\n",
        "# print(training_batches)\n",
        "# print('tn')\n",
        "# b = [random.choice(pairs) for _ in range(5)]\n",
        "# print(b)\n",
        "# i = 0\n",
        "# for batch in train_loader:\n",
        "#   print(batch)\n",
        "#   if(i==0):\n",
        "#     break"
      ],
      "metadata": {
        "id": "4T_tmg34QkcJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Epochs/\n",
        "total_epochs = 1\n",
        "batch_size = 64\n",
        "for epoch in range(total_epochs):\n",
        "  # training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "  #                   for _ in range(n_iteration)]\n",
        "  \n",
        "  #train\n",
        "  transformer.train()\n",
        "  print_counter=0\n",
        "  print_mul=0\n",
        "  for i,batch_pair in enumerate(train_loader):    \n",
        "    if i == len(train_loader) - 1:\n",
        "      continue  # Skip the last batch\n",
        "    tot_loss = 0\n",
        "    \n",
        "    #keep input_variable and variable lenght\n",
        "    # train_batch = training_batches[iter-1\n",
        "\n",
        "    input_variable, lengths, target_variable, mask, max_target_len = batch_pair\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(input_variable, target_variable)\n",
        "    logits = transformer(input_variable.to('cuda'), target_variable.to('cuda'), src_mask.to('cuda'), tgt_mask.to('cuda'),src_padding_mask.to('cuda'), tgt_padding_mask.to('cuda'), src_padding_mask.to('cuda'))\n",
        "    optimizer.zero_grad()\n",
        "    # Define special symbols and indices\n",
        "    UNK_token, PAD_token, SOS_token, EOS_token = 0, 1, 2, 3\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "    tgt_out = target_variable\n",
        "    logits = logits.to(torch.float32).to('cuda')  # Convert logits to floating-point data type\n",
        "    tgt_out = tgt_out.to(torch.int64).to('cuda')\n",
        "    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print_counter+=1\n",
        "    tot_loss += loss.item()\n",
        "    if(print_counter == 5):\n",
        "      \n",
        "      print(\"Epoch: {}| iterations complete: {} | loss at this iteration {}\".format(epoch, print_mul*print_counter,loss))\n",
        "      print_counter = 0\n",
        "  print('Epoch Training done, Now validating')\n",
        "\n",
        "  #validate\n",
        "  transformer.eval()\n",
        "  for batch_pair in val_loader:\n",
        "    \n",
        "    val_loss = 0\n",
        "    #keep input_variable and variable lenght\n",
        "    # train_batch = training_batches[iter-1\n",
        "\n",
        "    input_variable, lengths, target_variable, mask, max_target_len = batch_pair\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(input_variable, target_variable)\n",
        "    logits = transformer(input_variable.to('cuda'), target_variable.to('cuda'), src_mask.to('cuda'), tgt_mask.to('cuda'),src_padding_mask.to('cuda'), tgt_padding_mask.to('cuda'), src_padding_mask.to('cuda'))\n",
        "    optimizer.zero_grad()\n",
        "    # Define special symbols and indices\n",
        "    UNK_token, PAD_token, SOS_token, EOS_token = 0, 1, 2, 3\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "    tgt_out = target_variable\n",
        "    logits = logits.to(torch.float32).to('cuda')  # Convert logits to floating-point data type\n",
        "    tgt_out = tgt_out.to(torch.int64).to('cuda')\n",
        "    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "    val_loss += loss.item()\n",
        "    \n",
        "  print(\"Epoch: {}| Percent complete: {:.1f}%| Average loss: {:.4f}| Val loss: {:.4f}\".format(epoch, epoch / total_epochs * 100,tot_loss,val_loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUOMz5NMq0BJ",
        "outputId": "eb078aa5-4b9e-42b5-e0ca-bcb4e37180f6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0| iterations complete: 0 | loss at this iteration 7.742984294891357\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 7.290112018585205\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 6.928518295288086\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 6.553684711456299\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 6.023582458496094\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 5.99150276184082\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 5.6794562339782715\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 5.187742710113525\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 4.8398590087890625\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 4.667515277862549\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 4.285816669464111\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 4.307641506195068\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 3.9078054428100586\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 3.822504997253418\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 3.686656951904297\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 3.7225141525268555\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 3.4853157997131348\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 3.2414004802703857\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.8757247924804688\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.8910484313964844\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.789890766143799\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 3.0104494094848633\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.6483678817749023\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.572216510772705\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.591733455657959\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.364863634109497\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.313373565673828\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.4274423122406006\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.354382038116455\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.453169345855713\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.156294822692871\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.1171092987060547\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.3028194904327393\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.8700051307678223\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 2.3062825202941895\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.7981235980987549\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.9594166278839111\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.8461604118347168\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.617836356163025\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.9033374786376953\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.7195295095443726\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.84296452999115\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.4915313720703125\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.6542717218399048\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.497071623802185\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.5268430709838867\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.4535517692565918\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.626866102218628\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.7056407928466797\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.6954559087753296\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.5336146354675293\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.686630129814148\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.2256159782409668\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.5879210233688354\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.6767199039459229\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.5797324180603027\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.420845866203308\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.3337547779083252\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.2460309267044067\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.4410631656646729\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.2351911067962646\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.3090375661849976\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.4645248651504517\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.3508405685424805\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.3159574270248413\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.6451456546783447\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.4694327116012573\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9476925134658813\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.4381585121154785\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.273975133895874\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.3475970029830933\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.2753407955169678\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.4964711666107178\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.3425745964050293\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.1865500211715698\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.1067134141921997\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.1980035305023193\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.2067663669586182\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.117781400680542\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.1733554601669312\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.019935965538025\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.2353806495666504\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.3123282194137573\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.1670746803283691\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.1108249425888062\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9935582876205444\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.119127631187439\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.1226234436035156\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.0137351751327515\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.0785075426101685\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.126725435256958\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.0255956649780273\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.148258090019226\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.0055304765701294\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8834291696548462\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.178335189819336\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9530472755432129\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9744812846183777\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9869794845581055\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.279114842414856\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8207147717475891\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.0024476051330566\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.065299153327942\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8340943455696106\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.0060676336288452\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.0524506568908691\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9324984550476074\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8122786283493042\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.1560297012329102\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.0273593664169312\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9858368039131165\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.1089658737182617\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9560889601707458\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9075939655303955\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.1373504400253296\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9156337380409241\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9455657005310059\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8437570929527283\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8261280655860901\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8418949842453003\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.7031890153884888\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.0276989936828613\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8606131076812744\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8168897032737732\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9364218711853027\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9454268217086792\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.9507579803466797\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 1.272566795349121\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8633160591125488\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.953364908695221\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.739502489566803\n",
            "Epoch: 0| iterations complete: 0 | loss at this iteration 0.8777211308479309\n",
            "Epoch Training done, Now validating\n",
            "Epoch: 0| Percent complete: 0.0%| Average loss: 0.9677| Val loss: 0.4279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# Save the entire model\n",
        "torch.save(transformer.state_dict, 'model.pth')\n",
        "model_path = os.path.join(\"/content/gdrive/My Drive/data/model\", 'transformer.pt')\n",
        "model_path_state= os.path.join(\"/content/gdrive/My Drive/data/model_dict\", 'transformer_state.pt')\n",
        "# Save only the model state dictionary\n",
        "torch.save(transformer.state_dict(), model_path_state)\n",
        "torch.save(transformer,model_path )\n"
      ],
      "metadata": {
        "id": "LAFaV982u-M7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb1798c-4ecc-40b1-939e-1a0dac487903"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}